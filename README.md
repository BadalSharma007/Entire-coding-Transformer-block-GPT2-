# Entire-coding-Transformer-block-GPT2-
GPT Transformer Block in PyTorch  A minimal GPT-style Transformer with Self-Attention, FFN, LayerNorm, and GPT-2 tokenization (tiktoken). Modular and scalable for NLP and LLM research. ðŸš€
